"""
LLM Status API - Resilient status reporting
"""
import logging
from flask import Blueprint, jsonify, current_app

logger = logging.getLogger(__name__)

# Create blueprint
bp = Blueprint("llm_status_api", __name__, url_prefix="/api/llm")

def _collect_providers() -> dict:
    """
    Build a synthetic provider report.
    Never raises; if not configured, returns configured=False, providers=[]
    """
    try:
        cfg = current_app.config or {}
        feature = cfg.get("FEATURE_LLM_API", True)
        if not feature:
            return {"configured": False, "providers": [], "ok": False, "details": "disabled"}

        # Example heuristics: treat presence of OPENAI_API_KEY (or similar) as "configured"
        configured = bool(cfg.get("OPENAI_API_KEY") or cfg.get("ANTHROPIC_API_KEY"))
        if not configured:
            return {"configured": False, "providers": [], "ok": False, "details": "not_configured"}

        # If configured, pretend it's OK unless a circuit flag is set
        # (Adjust to your real validator if present)
        providers = [{
            "name": "openai" if cfg.get("OPENAI_API_KEY") else "unknown",
            "active": True,
            "last_ok": None,
            "failure_count": 0,
            "circuit_state": "closed",
            "avg_latency_ms": None,
            "today_requests": 0,
            "today_tokens": 0,
            "configured": True,
        }]
        
        # Get routing information if available
        routing_info = {}
        try:
            from src.llm.providers import LLMProviderManager
            provider_manager = LLMProviderManager()
            routing_info = provider_manager.get_routing_info()
        except ImportError:
            pass
        
        return {
            "configured": True, 
            "providers": providers, 
            "ok": True, 
            "details": "ok",
            "routing": routing_info
        }
    except Exception as e:
        logger.warning(f"Error collecting LLM providers: {e}")
        return {"configured": False, "providers": [], "ok": False, "details": f"error:{type(e).__name__}"}

@bp.route("/status", methods=["GET"])
def status():
    """Get LLM status - never throws, always returns 200"""
    try:
        report = _collect_providers()
    except Exception as e:
        # absolute guardrail â€” never raise
        logger.error(f"LLM status endpoint error: {e}")
        report = {"configured": False, "providers": [], "ok": False, "details": f"error:{type(e).__name__}"}
    return jsonify(report), 200

def get_status_summary() -> dict:
    """Tiny wrapper so readiness can import without flask request context."""
    try:
        r = _collect_providers()
        return {"configured": r.get("configured", False),
                "ok": r.get("ok", False),
                "details": r.get("details", "not_configured")}
    except Exception as e:
        logger.warning(f"Error getting LLM status summary: {e}")
        return {"configured": False, "ok": False, "details": "not_configured"}

# Additional endpoints for compatibility
@bp.route("/provider/status", methods=["GET"])
def provider_status():
    """Legacy endpoint - redirects to main status"""
    return status()

@bp.route("/metrics", methods=["GET"])
def metrics():
    """LLM metrics endpoint"""
    try:
        report = _collect_providers()
        metrics_data = {
            "providers": len(report.get("providers", [])),
            "configured": report.get("configured", False),
            "ok": report.get("ok", False)
        }
        return jsonify(metrics_data), 200
    except Exception as e:
        logger.error(f"LLM metrics endpoint error: {e}")
        return jsonify({"error": "metrics_unavailable"}), 200
